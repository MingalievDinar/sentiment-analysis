{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim - is to develop a model that will give accurate predictions for the customer's test sample, but the training sample is not given -  to be collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import bs4\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "\n",
    "# allows to open more files in the same time\n",
    "import resource\n",
    "resource.setrlimit(resource.RLIMIT_NOFILE, (10000,-1))\n",
    "# resource.getrlimit(resource.RLIMIT_NOFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test.csv - Test data with revies\n",
    "with open('test.csv', 'r') as file:\n",
    "    a = file.read()\n",
    "parser = bs4.BeautifulSoup(a, 'lxml')\n",
    "test_data = [item.get_text(strip=True) for item in parser.find_all('review')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internet shop links have similar structure.\n",
    "Here we are 1) parsing all necessary links, 2) parsing all feedbacks and number of star for each feedback (5* - is good, 1* - is bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting parse_mobile_links.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile parse_mobile_links.py\n",
    "import requests\n",
    "import bs4\n",
    "from multiprocessing import Pool\n",
    "import codecs\n",
    "import time\n",
    "hdr = {\"User-Agent\": \"Mozilla/5.0\",\n",
    "       \"Accept-Encoding\":\"gzip, deflate, sdch\",\n",
    "       \"Accept-Language\":\"ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4\"}\n",
    "\n",
    "def parse_page(url):\n",
    "    n = 0\n",
    "    s = 0\n",
    "    COND_ALLOW_PARS = False\n",
    "    while not COND_ALLOW_PARS:\n",
    "        text = requests.get(url, headers = hdr).text\n",
    "        parser = bs4.BeautifulSoup(text, 'lxml')\n",
    "        tag_h1 = parser.find('h1')\n",
    "        COND_ALLOW_PARS = (tag_h1.get_text()[:10] == u'\\u041c\\u043e\\u0431\\u0438\\u043b\\u044c\\u043d\\u044b\\u0435 ')\n",
    "        print \"trying to parse \" + url\n",
    "        print tag_h1\n",
    "        print \"COND_ALLOW_PARS :{}\".format(COND_ALLOW_PARS)\n",
    "        s += 5+n\n",
    "        time.sleep(5+n)\n",
    "        n += 1\n",
    "        print \"Sleep now: {} sec. Total sleep: {} sec\".format(n+5, s)\n",
    "    x = parser.findAll('div', attrs={'class':'citate'})\n",
    "    z = [item.find('a').get('href') for item in x]\n",
    "    print \"parssed link\" + url\n",
    "    time.sleep(5)\n",
    "    return z\n",
    "\n",
    "p = Pool(1)\n",
    "url_list = ['http://irecommend.ru/category/mobilnye-telefony?page=' + str(n) for n in range(0, 162)]\n",
    "    \n",
    "if __name__ == '__main__':    \n",
    "    map_results = p.map(parse_page, url_list)\n",
    "    reduce_results = reduce(lambda x,y: x + y, map_results)\n",
    "    with codecs.open('parsing_results2.txt', 'w', 'utf-8') as output_file:\n",
    "        print >> output_file, u'\\n'.join(reduce_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of links: 3222\n"
     ]
    }
   ],
   "source": [
    "with open('parsing_results2.txt', 'r') as file:\n",
    "    l = file.read()\n",
    "short_links = l.split('\\n')\n",
    "print 'Number of links: {}'.format(len(short_links))\n",
    "links = ['http://irecommend.ru' + i for i in short_links]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#%%writefile parse_mobile_data.py\n",
    "import requests\n",
    "import bs4\n",
    "#from multiprocessing import Pool\n",
    "#import codecs\n",
    "import time\n",
    "#import random\n",
    "\n",
    "with open('parsing_results2.txt', 'r') as file:\n",
    "    l = file.read()\n",
    "short_links = l.split('\\n')\n",
    "url_list = ['http://irecommend.ru' + i for i in short_links]\n",
    "\n",
    "hdr = {\"User-Agent\": \"Mozilla/5.0\",\n",
    "       \"Accept-Encoding\":\"gzip, deflate, sdch\",\n",
    "       \"Accept-Language\":\"ru-RU,ru;q=0.8,en-US;q=0.6,en;q=0.4\"}\n",
    "\n",
    "def parse_page(url):\n",
    "    n = 0\n",
    "    s = 0\n",
    "    \n",
    "    COND_ALLOW_PARS = True\n",
    "    # sleep while banned\n",
    "    while COND_ALLOW_PARS:\n",
    "        text = requests.get(url, headers = hdr).text\n",
    "        parser = bs4.BeautifulSoup(text, 'lxml')\n",
    "        tag_h1 = parser.find('h1')\n",
    "        COND_ALLOW_PARS = (tag_h1.get_text()[:28] == u'\\u041d\\u0430 \\u0441\\u0430\\u0439\\u0442\\u0435 \\u0432\\u0435\\u0434\\u0443\\u0442\\u0441\\u044f \\u0442\\u0435\\u0445\\u043d\\u0438\\u0447\\u0435\\u0441\\u043a\\u0438\\u0435')\n",
    "        #print \"trying to parse \" + url\n",
    "        #print tag_h1\n",
    "        #print \"COND_ALLOW_PARS :{}\".format(COND_ALLOW_PARS)\n",
    "        s += 5+n\n",
    "        time.sleep(5+n)\n",
    "        n += 1\n",
    "        #print \"Sleep now: {} sec. Total sleep: {} sec\".format(n+5, s)\n",
    "    try:\n",
    "        rating = float(parser.find('span', attrs={'class':'on rating', 'itemprop':'ratingValue'}).get_text())\n",
    "    except:\n",
    "        rating = -1\n",
    "    try:\n",
    "        text = parser.find('div', attrs={'class':'description hasinlineimage', 'itemprop':'reviewBody'}).get_text().replace('\\n', \"\").replace('\\t', '')\n",
    "    except:\n",
    "        try:\n",
    "            # different class values\n",
    "            text = parser.find('div', attrs={'class':'description', 'itemprop':'reviewBody'}).get_text().replace('\\n', \"\").replace('\\t', '')\n",
    "        except:\n",
    "            text = \"\"\n",
    "    z = {'rating' : rating, 'feedback':text}\n",
    "    #print \"parssed link\" + url\n",
    "    sleep_s = 5 + random.randint(0,10)/10.0\n",
    "    #print 'Sleep after parsing {} sec'.format(sleep_s)\n",
    "    time.sleep(sleep_s)\n",
    "    return z\n",
    "ind = 0\n",
    "data = list()\n",
    "for url in url_list:\n",
    "    res = parse_page(url)\n",
    "    ind += 1\n",
    "    data.append(res)\n",
    "    print ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "from nltk.stem.snowball import RussianStemmer\n",
    "from nltk import word_tokenize\n",
    "\n",
    "table = { ord(char): ord(' ') for char in string.punctuation }\n",
    "stemmer = RussianStemmer() \n",
    "def preprocess(doc): \n",
    "    words = [stemmer.stem(t) for t in doc.lower().translate(table).split()]            \n",
    "    return ' '.join(words)\n",
    "\n",
    "train_data_X = [preprocess(item) for item in test_data]\n",
    "data.feedback = [preprocess(item) for item in data.feedback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_original\n",
    "data = data_original[data_original.rating<>-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feedback_cleaned = [\" \".join(re.findall('[^0-9/.:,;/s)(~+-?!\"]+', item)) for item in data.feedback.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_data_x = [\" \".join(re.findall('[^0-9/.:,;/s)(~+-?!\"]+', item)) for item in test_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data['feedback_cleaned'] = feedback_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 2\n",
    "def rating_cleaning(rat):\n",
    "    if rat >= a:\n",
    "        return 1\n",
    "    elif rat<= b:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "data['rating_cleaned'] = map(rating_cleaning, data.rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feedback</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Всем привет!Начну с того, что я именно тот че...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Купила смартфон Samsung Galaxy J7 более полуг...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Изначально в семье появился один такой телефо...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Купила на Алиэкспресс телефон. На этот раз дл...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Классный телефончик без преувеличении могу ск...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            feedback  rating  rating_cleaned\n",
       "0   Всем привет!Начну с того, что я именно тот че...     4.0              -1\n",
       "1   Купила смартфон Samsung Galaxy J7 более полуг...     4.0              -1\n",
       "2   Изначально в семье появился один такой телефо...     5.0               1\n",
       "3   Купила на Алиэкспресс телефон. На этот раз дл...     5.0               1\n",
       "4   Классный телефончик без преувеличении могу ск...     5.0               1"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data1 = data[['feedback', 'rating_cleaned']]\n",
    "#data1 = data[['feedback_cleaned', 'rating_cleaned']]\n",
    "#data1.columns = ['feedback', 'rating', 'feedback_cleaned', 'rating_cleaned']\n",
    "data1 = data1[data1.rating_cleaned <> -1]\n",
    "print data1.rating_cleaned.value_counts()\n",
    "print data1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data2 = data[['feedback', 'rating', 'feedback_cleaned', 'rating_cleaned2']]\n",
    "#data2.columns = ['feedback', 'rating', 'feedback_cleaned', 'rating_cleaned']\n",
    "#data2 = data2[data2.rating_cleaned <> -1]\n",
    "#data2.rating_cleaned.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data for model development\n",
    "X = data1.feedback.tolist()\n",
    "#X = data1.feedback_cleaned.tolist()\n",
    "y = data1.rating_cleaned.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Search for the best Classifier. DO NOT START - it takes a long time.\n",
    "We use Vectorizer: CountVectorizer, TfidfVectorizer and models: SVC, SGD, logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf': SGDClassifier(alpha=0.0001, average=False, class_weight='balanced',\n",
      "       epsilon=0.1, eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False), 'count_vec': TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=0.99, max_features=None, min_df=1,\n",
      "        ngram_range=(2, 2), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=[u'i', u'me', u'my', u'myself', u'we', u'our', u'ours', u'ourselves', u'you', u'your', u'yours', u'yourself', u'yourselves', u'he', u'him', u'his', u'himself', u'she', u'her', u'hers', u'herself', u'it', u'its', u'itself', u'they', u'them', u'their', u'theirs', u'themselves', u'what', u'w...n', u'ma', u'mightn', u'mustn', u'needn', u'shan', u'shouldn', u'wasn', u'weren', u'won', u'wouldn'],\n",
      "        strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)}\n"
     ]
    }
   ],
   "source": [
    "SVC = LinearSVC()\n",
    "SGD = SGDClassifier()\n",
    "logistic = LogisticRegression()\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "count_vec = CountVectorizer()\n",
    "TFDF = TfidfVectorizer()\n",
    "pipe_all = Pipeline(steps=[('count_vec', count_vec), ('clf', SVC)])\n",
    "pipe_all.set_params(count_vec__min_df= 2.0, count_vec__max_df=0.99, count_vec__stop_words = 'english',\n",
    "                   count_vec__ngram_range = (1,1), count_vec__analyzer = 'word',\n",
    "                   clf__class_weight='balanced')\n",
    "\n",
    "params = dict(\n",
    "    count_vec = [count_vec, TFDF],\n",
    "    count_vec__min_df = [0, 1, 2, 3],\n",
    "    count_vec__max_df = [0.99, 0.98, 0.95, 1.0],\n",
    "    count_vec__stop_words = ['english', stop, None],\n",
    "    count_vec__analyzer = ['word'],# 'char_wb'],\n",
    "    count_vec__ngram_range=[(1,1), (2,2), (1, 2), (2,3), (1,3), (3,3)],\n",
    "    clf = [SVC, SGD, logistic],\n",
    "    clf__class_weight=['balanced', None])\n",
    "\n",
    "grid_search = GridSearchCV(pipe_all, param_grid=params, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "mean_scores_test = np.array(grid_search.cv_results_['mean_test_score'])\n",
    "result_test = mean_scores_test\n",
    "print grid_search.best_estimator_.named_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('russian')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best result is on SVC classifier with TFidF. Cross validaiotn on 3 folds showed the result 91.2%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91185888892442968"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVC = LinearSVC(class_weight='balanced', C=0.9)\n",
    "SGD = SGDClassifier(class_weight='balanced')\n",
    "logistic = LogisticRegression(class_weight='balanced', C=0.9, n_jobs=4)\n",
    "\n",
    "estimator = SVC\n",
    "\n",
    "TFDF = TfidfVectorizer(min_df= 2, max_df=0.99, stop_words = stop_words, ngram_range = (1,3), analyzer = 'word')\n",
    "pipe_best1 = Pipeline(steps=[('count_vec', TFDF), ('clf', estimator)])\n",
    "cross_val_score(pipe_best1, X, y, cv=3).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vec', TfidfVectorizer(analyzer='word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=0.99, max_features=None, min_df=2,\n",
       "        ngram_range=(1, 3), norm=u'l2', preprocessor=None, smooth_idf...ax_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 586,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_best1.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = pipe_best1.predict(train_data_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# to save the results\n",
    "z = ['pos' if item == 1 else 'neg' for item in results]\n",
    "z = pd.DataFrame(z)\n",
    "z.columns = ['y']\n",
    "z.index.name = 'id'\n",
    "file_name = 'pipe_SGD_TFDF1_tokenizer15.csv'\n",
    "z.to_csv(path_or_buf=file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
